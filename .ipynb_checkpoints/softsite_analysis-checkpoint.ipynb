{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 158,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import sum, mean, udf, col, round, max, greatest, count\n",
    "from pyspark.sql.types import DoubleType\n",
    "from pyspark.sql.functions import rank, col\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import time\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ! curl -O https://raw.githubusercontent.com/NYCPlanning/db-pluto/master/pluto_build/data/dcp_zoning_maxfar.csv\n",
    "# ! mv dcp_zoning_maxfar.csv data/dcp_zoning_maxfar.csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = spark.read.csv('data/pluto.csv', header=True)\n",
    "new_far = spark.read.csv('data/dcp_zoning_maxfar.csv', header=True)\n",
    "df = df.select([col(A).alias(A.lower()) for A in df.columns])\n",
    "\n",
    "#type conversion, '-' --> null\n",
    "for A in ['residfar','commfar']:\n",
    "    new_far = new_far.withColumn(A, col(A).cast(DoubleType()))\n",
    "    df = df.withColumn(A, col(A).cast(DoubleType()))\n",
    "\n",
    "#create two copies\n",
    "new_far1 = new_far.select([col(A).alias(A.lower()+'_1') for A in ['zonedist','residfar','commfar']])\n",
    "new_far2 = new_far.select([col(A).alias(A.lower()+'_2') for A in ['zonedist','residfar','commfar']])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [],
   "source": [
    "@udf\n",
    "def pick_value(A,B):\n",
    "    if not A: \n",
    "        return B\n",
    "    else: \n",
    "        return A"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df.join(new_far1, df['zonedist1'] == new_far1['zonedist_1'], how='left')\\\n",
    "       .join(new_far2, df['zonedist2'] == new_far2['zonedist_2'], how='left')\n",
    "       .withColumn('residfar_new', pick_value(col('residfar_1'), col('residfar_2')))\\\n",
    "       .withColumn('commfar_new', pick_value(col('commfar_1'), col('commfar_2')))\n",
    "       .withColumn('maxfar', greatest(col('residfar'), col('commfar')))\\\n",
    "       .withColumn('pctunbuilt', ((col('maxfar') - col('builtfar'))/col('maxfar')))\\\n",
    "       .withColumn('maxfar_new', greatest(col('residfar_new'), col('commfar_new')))\\\n",
    "       .withColumn('pctunbuilt_new', ((col('maxfar_new') - col('builtfar'))/col('maxfar_new')))\\\n",
    "       .filter(col('unitsres').cast(DoubleType()) <= 6)\\\n",
    "       .filter(col('landmark').isNull())\\\n",
    "       .filter(col('irrlotcode') != 'Y')\\\n",
    "       .filter(~col ('bldgclass').like('M%'))\\\n",
    "       .filter(col('landuse') != '08')\\\n",
    "       .filter(col('easements').cast(DoubleType()) <= 0)\\"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+\n",
      "|increase|\n",
      "+--------+\n",
      "|    1472|\n",
      "+--------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.select('maxfar', 'maxfar_new')\\\n",
    "  .filter(col('maxfar') < col('maxfar_new')).agg(count('maxfar_new').alias('increase')).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 153,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+\n",
      "|increase|\n",
      "+--------+\n",
      "|   43285|\n",
      "+--------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.select('maxfar', 'maxfar_new')\\\n",
    "  .filter((col('maxfar') > col('maxfar_new')) &\n",
    "          (col('pctunbuilt') > 0.5))\\\n",
    "    .agg(count('maxfar_new').alias('increase'))\\\n",
    ".show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 154,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+\n",
      "|increase|\n",
      "+--------+\n",
      "|    2821|\n",
      "+--------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.select('maxfar', 'maxfar_new')\\\n",
    "  .filter((col('maxfar') > col('maxfar_new')) &\n",
    "          (col('pctunbuilt') == 0.5))\\\n",
    "    .agg(count('maxfar_new').alias('increase'))\\\n",
    ".show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 155,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+\n",
      "|increase|\n",
      "+--------+\n",
      "|  268385|\n",
      "+--------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.select('maxfar', 'maxfar_new')\\\n",
    "  .filter((col('maxfar') > col('maxfar_new')) &\n",
    "          (col('pctunbuilt') < 0.5))\\\n",
    "    .agg(count('maxfar_new').alias('increase'))\\\n",
    ".show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 162,
   "metadata": {},
   "outputs": [],
   "source": [
    "@udf\n",
    "def Larger(A,B): \n",
    "    if A>B: \n",
    "        return 1\n",
    "    else: \n",
    "        return 0\n",
    "\n",
    "@udf\n",
    "def Equal(A,B): \n",
    "    if A==B: \n",
    "        return 1\n",
    "    else: \n",
    "        return 0\n",
    "    \n",
    "@udf\n",
    "def Less(A,B): \n",
    "    if A<B: \n",
    "        return 1\n",
    "    else: \n",
    "        return 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 166,
   "metadata": {},
   "outputs": [
    {
     "ename": "Py4JJavaError",
     "evalue": "An error occurred while calling o5389.showString.\n: org.apache.spark.SparkException: Job aborted due to stage failure: Task 3 in stage 420.0 failed 1 times, most recent failure: Lost task 3.0 in stage 420.0 (TID 3216, localhost, executor driver): org.apache.spark.api.python.PythonException: Traceback (most recent call last):\n  File \"/Users/baiyue/spark-2.4.0-bin-hadoop2.7/python/lib/pyspark.zip/pyspark/worker.py\", line 372, in main\n    process()\n  File \"/Users/baiyue/spark-2.4.0-bin-hadoop2.7/python/lib/pyspark.zip/pyspark/worker.py\", line 367, in process\n    serializer.dump_stream(func(split_index, iterator), outfile)\n  File \"/Users/baiyue/spark-2.4.0-bin-hadoop2.7/python/lib/pyspark.zip/pyspark/serializers.py\", line 342, in dump_stream\n    self.serializer.dump_stream(self._batched(iterator), stream)\n  File \"/Users/baiyue/spark-2.4.0-bin-hadoop2.7/python/lib/pyspark.zip/pyspark/serializers.py\", line 141, in dump_stream\n    for obj in iterator:\n  File \"/Users/baiyue/spark-2.4.0-bin-hadoop2.7/python/lib/pyspark.zip/pyspark/serializers.py\", line 331, in _batched\n    for item in iterator:\n  File \"<string>\", line 1, in <lambda>\n  File \"/Users/baiyue/spark-2.4.0-bin-hadoop2.7/python/lib/pyspark.zip/pyspark/worker.py\", line 80, in <lambda>\n    return lambda *a: f(*a)\n  File \"/Users/baiyue/spark-2.4.0-bin-hadoop2.7/python/lib/pyspark.zip/pyspark/util.py\", line 99, in wrapper\n    return f(*args, **kwargs)\n  File \"<ipython-input-162-90c40e18ab34>\", line 3, in Larger\nTypeError: '>' not supported between instances of 'float' and 'str'\n\n\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.handlePythonException(PythonRunner.scala:452)\n\tat org.apache.spark.sql.execution.python.PythonUDFRunner$$anon$1.read(PythonUDFRunner.scala:81)\n\tat org.apache.spark.sql.execution.python.PythonUDFRunner$$anon$1.read(PythonUDFRunner.scala:64)\n\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.hasNext(PythonRunner.scala:406)\n\tat org.apache.spark.InterruptibleIterator.hasNext(InterruptibleIterator.scala:37)\n\tat scala.collection.Iterator$$anon$12.hasNext(Iterator.scala:440)\n\tat scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:409)\n\tat scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:409)\n\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage5.agg_doAggregateWithoutKey_0$(Unknown Source)\n\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage5.processNext(Unknown Source)\n\tat org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43)\n\tat org.apache.spark.sql.execution.WholeStageCodegenExec$$anonfun$11$$anon$1.hasNext(WholeStageCodegenExec.scala:619)\n\tat scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:409)\n\tat org.apache.spark.shuffle.sort.BypassMergeSortShuffleWriter.write(BypassMergeSortShuffleWriter.java:125)\n\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:99)\n\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:55)\n\tat org.apache.spark.scheduler.Task.run(Task.scala:121)\n\tat org.apache.spark.executor.Executor$TaskRunner$$anonfun$10.apply(Executor.scala:402)\n\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1360)\n\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:408)\n\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n\tat java.lang.Thread.run(Thread.java:748)\n\nDriver stacktrace:\n\tat org.apache.spark.scheduler.DAGScheduler.org$apache$spark$scheduler$DAGScheduler$$failJobAndIndependentStages(DAGScheduler.scala:1887)\n\tat org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1875)\n\tat org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1874)\n\tat scala.collection.mutable.ResizableArray$class.foreach(ResizableArray.scala:59)\n\tat scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:48)\n\tat org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:1874)\n\tat org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:926)\n\tat org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:926)\n\tat scala.Option.foreach(Option.scala:257)\n\tat org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:926)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:2108)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2057)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2046)\n\tat org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:49)\n\tat org.apache.spark.scheduler.DAGScheduler.runJob(DAGScheduler.scala:737)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2061)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2082)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2101)\n\tat org.apache.spark.sql.execution.SparkPlan.executeTake(SparkPlan.scala:365)\n\tat org.apache.spark.sql.execution.CollectLimitExec.executeCollect(limit.scala:38)\n\tat org.apache.spark.sql.Dataset.org$apache$spark$sql$Dataset$$collectFromPlan(Dataset.scala:3384)\n\tat org.apache.spark.sql.Dataset$$anonfun$head$1.apply(Dataset.scala:2545)\n\tat org.apache.spark.sql.Dataset$$anonfun$head$1.apply(Dataset.scala:2545)\n\tat org.apache.spark.sql.Dataset$$anonfun$53.apply(Dataset.scala:3365)\n\tat org.apache.spark.sql.execution.SQLExecution$$anonfun$withNewExecutionId$1.apply(SQLExecution.scala:78)\n\tat org.apache.spark.sql.execution.SQLExecution$.withSQLConfPropagated(SQLExecution.scala:125)\n\tat org.apache.spark.sql.execution.SQLExecution$.withNewExecutionId(SQLExecution.scala:73)\n\tat org.apache.spark.sql.Dataset.withAction(Dataset.scala:3364)\n\tat org.apache.spark.sql.Dataset.head(Dataset.scala:2545)\n\tat org.apache.spark.sql.Dataset.take(Dataset.scala:2759)\n\tat org.apache.spark.sql.Dataset.getRows(Dataset.scala:255)\n\tat org.apache.spark.sql.Dataset.showString(Dataset.scala:292)\n\tat sun.reflect.GeneratedMethodAccessor61.invoke(Unknown Source)\n\tat sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n\tat java.lang.reflect.Method.invoke(Method.java:498)\n\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\n\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357)\n\tat py4j.Gateway.invoke(Gateway.java:282)\n\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\n\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\n\tat py4j.GatewayConnection.run(GatewayConnection.java:238)\n\tat java.lang.Thread.run(Thread.java:748)\nCaused by: org.apache.spark.api.python.PythonException: Traceback (most recent call last):\n  File \"/Users/baiyue/spark-2.4.0-bin-hadoop2.7/python/lib/pyspark.zip/pyspark/worker.py\", line 372, in main\n    process()\n  File \"/Users/baiyue/spark-2.4.0-bin-hadoop2.7/python/lib/pyspark.zip/pyspark/worker.py\", line 367, in process\n    serializer.dump_stream(func(split_index, iterator), outfile)\n  File \"/Users/baiyue/spark-2.4.0-bin-hadoop2.7/python/lib/pyspark.zip/pyspark/serializers.py\", line 342, in dump_stream\n    self.serializer.dump_stream(self._batched(iterator), stream)\n  File \"/Users/baiyue/spark-2.4.0-bin-hadoop2.7/python/lib/pyspark.zip/pyspark/serializers.py\", line 141, in dump_stream\n    for obj in iterator:\n  File \"/Users/baiyue/spark-2.4.0-bin-hadoop2.7/python/lib/pyspark.zip/pyspark/serializers.py\", line 331, in _batched\n    for item in iterator:\n  File \"<string>\", line 1, in <lambda>\n  File \"/Users/baiyue/spark-2.4.0-bin-hadoop2.7/python/lib/pyspark.zip/pyspark/worker.py\", line 80, in <lambda>\n    return lambda *a: f(*a)\n  File \"/Users/baiyue/spark-2.4.0-bin-hadoop2.7/python/lib/pyspark.zip/pyspark/util.py\", line 99, in wrapper\n    return f(*args, **kwargs)\n  File \"<ipython-input-162-90c40e18ab34>\", line 3, in Larger\nTypeError: '>' not supported between instances of 'float' and 'str'\n\n\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.handlePythonException(PythonRunner.scala:452)\n\tat org.apache.spark.sql.execution.python.PythonUDFRunner$$anon$1.read(PythonUDFRunner.scala:81)\n\tat org.apache.spark.sql.execution.python.PythonUDFRunner$$anon$1.read(PythonUDFRunner.scala:64)\n\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.hasNext(PythonRunner.scala:406)\n\tat org.apache.spark.InterruptibleIterator.hasNext(InterruptibleIterator.scala:37)\n\tat scala.collection.Iterator$$anon$12.hasNext(Iterator.scala:440)\n\tat scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:409)\n\tat scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:409)\n\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage5.agg_doAggregateWithoutKey_0$(Unknown Source)\n\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage5.processNext(Unknown Source)\n\tat org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43)\n\tat org.apache.spark.sql.execution.WholeStageCodegenExec$$anonfun$11$$anon$1.hasNext(WholeStageCodegenExec.scala:619)\n\tat scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:409)\n\tat org.apache.spark.shuffle.sort.BypassMergeSortShuffleWriter.write(BypassMergeSortShuffleWriter.java:125)\n\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:99)\n\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:55)\n\tat org.apache.spark.scheduler.Task.run(Task.scala:121)\n\tat org.apache.spark.executor.Executor$TaskRunner$$anonfun$10.apply(Executor.scala:402)\n\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1360)\n\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:408)\n\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n\t... 1 more\n",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mPy4JJavaError\u001b[0m                             Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-166-24b6579a18ee>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m df.select('maxfar', 'maxfar_new')  .withColumn('Larger', Larger(col('maxfar'),col('maxfar_new')))  .withColumn('Equal', Equal(col('maxfar'),col('maxfar_new')))  .withColumn('Less', Less(col('maxfar'),col('maxfar_new')))  .agg(sum(col('Larger')).alias('decrease'),\n\u001b[1;32m      2\u001b[0m       \u001b[0msum\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcol\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'Equal'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0malias\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'no change'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m       sum(col('Less')).alias('increase')).show()\n\u001b[0m",
      "\u001b[0;32m~/spark-2.4.0-bin-hadoop2.7/python/pyspark/sql/dataframe.py\u001b[0m in \u001b[0;36mshow\u001b[0;34m(self, n, truncate, vertical)\u001b[0m\n\u001b[1;32m    376\u001b[0m         \"\"\"\n\u001b[1;32m    377\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtruncate\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbool\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mtruncate\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 378\u001b[0;31m             \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jdf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshowString\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m20\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvertical\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    379\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    380\u001b[0m             \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jdf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshowString\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtruncate\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvertical\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/spark-2.4.0-bin-hadoop2.7/python/lib/py4j-0.10.7-src.zip/py4j/java_gateway.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args)\u001b[0m\n\u001b[1;32m   1255\u001b[0m         \u001b[0manswer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgateway_client\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msend_command\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcommand\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1256\u001b[0m         return_value = get_return_value(\n\u001b[0;32m-> 1257\u001b[0;31m             answer, self.gateway_client, self.target_id, self.name)\n\u001b[0m\u001b[1;32m   1258\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1259\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mtemp_arg\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mtemp_args\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/spark-2.4.0-bin-hadoop2.7/python/pyspark/sql/utils.py\u001b[0m in \u001b[0;36mdeco\u001b[0;34m(*a, **kw)\u001b[0m\n\u001b[1;32m     61\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mdeco\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkw\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     62\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 63\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkw\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     64\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mpy4j\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mprotocol\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mPy4JJavaError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     65\u001b[0m             \u001b[0ms\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjava_exception\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtoString\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/spark-2.4.0-bin-hadoop2.7/python/lib/py4j-0.10.7-src.zip/py4j/protocol.py\u001b[0m in \u001b[0;36mget_return_value\u001b[0;34m(answer, gateway_client, target_id, name)\u001b[0m\n\u001b[1;32m    326\u001b[0m                 raise Py4JJavaError(\n\u001b[1;32m    327\u001b[0m                     \u001b[0;34m\"An error occurred while calling {0}{1}{2}.\\n\"\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 328\u001b[0;31m                     format(target_id, \".\", name), value)\n\u001b[0m\u001b[1;32m    329\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    330\u001b[0m                 raise Py4JError(\n",
      "\u001b[0;31mPy4JJavaError\u001b[0m: An error occurred while calling o5389.showString.\n: org.apache.spark.SparkException: Job aborted due to stage failure: Task 3 in stage 420.0 failed 1 times, most recent failure: Lost task 3.0 in stage 420.0 (TID 3216, localhost, executor driver): org.apache.spark.api.python.PythonException: Traceback (most recent call last):\n  File \"/Users/baiyue/spark-2.4.0-bin-hadoop2.7/python/lib/pyspark.zip/pyspark/worker.py\", line 372, in main\n    process()\n  File \"/Users/baiyue/spark-2.4.0-bin-hadoop2.7/python/lib/pyspark.zip/pyspark/worker.py\", line 367, in process\n    serializer.dump_stream(func(split_index, iterator), outfile)\n  File \"/Users/baiyue/spark-2.4.0-bin-hadoop2.7/python/lib/pyspark.zip/pyspark/serializers.py\", line 342, in dump_stream\n    self.serializer.dump_stream(self._batched(iterator), stream)\n  File \"/Users/baiyue/spark-2.4.0-bin-hadoop2.7/python/lib/pyspark.zip/pyspark/serializers.py\", line 141, in dump_stream\n    for obj in iterator:\n  File \"/Users/baiyue/spark-2.4.0-bin-hadoop2.7/python/lib/pyspark.zip/pyspark/serializers.py\", line 331, in _batched\n    for item in iterator:\n  File \"<string>\", line 1, in <lambda>\n  File \"/Users/baiyue/spark-2.4.0-bin-hadoop2.7/python/lib/pyspark.zip/pyspark/worker.py\", line 80, in <lambda>\n    return lambda *a: f(*a)\n  File \"/Users/baiyue/spark-2.4.0-bin-hadoop2.7/python/lib/pyspark.zip/pyspark/util.py\", line 99, in wrapper\n    return f(*args, **kwargs)\n  File \"<ipython-input-162-90c40e18ab34>\", line 3, in Larger\nTypeError: '>' not supported between instances of 'float' and 'str'\n\n\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.handlePythonException(PythonRunner.scala:452)\n\tat org.apache.spark.sql.execution.python.PythonUDFRunner$$anon$1.read(PythonUDFRunner.scala:81)\n\tat org.apache.spark.sql.execution.python.PythonUDFRunner$$anon$1.read(PythonUDFRunner.scala:64)\n\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.hasNext(PythonRunner.scala:406)\n\tat org.apache.spark.InterruptibleIterator.hasNext(InterruptibleIterator.scala:37)\n\tat scala.collection.Iterator$$anon$12.hasNext(Iterator.scala:440)\n\tat scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:409)\n\tat scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:409)\n\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage5.agg_doAggregateWithoutKey_0$(Unknown Source)\n\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage5.processNext(Unknown Source)\n\tat org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43)\n\tat org.apache.spark.sql.execution.WholeStageCodegenExec$$anonfun$11$$anon$1.hasNext(WholeStageCodegenExec.scala:619)\n\tat scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:409)\n\tat org.apache.spark.shuffle.sort.BypassMergeSortShuffleWriter.write(BypassMergeSortShuffleWriter.java:125)\n\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:99)\n\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:55)\n\tat org.apache.spark.scheduler.Task.run(Task.scala:121)\n\tat org.apache.spark.executor.Executor$TaskRunner$$anonfun$10.apply(Executor.scala:402)\n\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1360)\n\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:408)\n\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n\tat java.lang.Thread.run(Thread.java:748)\n\nDriver stacktrace:\n\tat org.apache.spark.scheduler.DAGScheduler.org$apache$spark$scheduler$DAGScheduler$$failJobAndIndependentStages(DAGScheduler.scala:1887)\n\tat org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1875)\n\tat org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1874)\n\tat scala.collection.mutable.ResizableArray$class.foreach(ResizableArray.scala:59)\n\tat scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:48)\n\tat org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:1874)\n\tat org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:926)\n\tat org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:926)\n\tat scala.Option.foreach(Option.scala:257)\n\tat org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:926)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:2108)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2057)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2046)\n\tat org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:49)\n\tat org.apache.spark.scheduler.DAGScheduler.runJob(DAGScheduler.scala:737)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2061)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2082)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2101)\n\tat org.apache.spark.sql.execution.SparkPlan.executeTake(SparkPlan.scala:365)\n\tat org.apache.spark.sql.execution.CollectLimitExec.executeCollect(limit.scala:38)\n\tat org.apache.spark.sql.Dataset.org$apache$spark$sql$Dataset$$collectFromPlan(Dataset.scala:3384)\n\tat org.apache.spark.sql.Dataset$$anonfun$head$1.apply(Dataset.scala:2545)\n\tat org.apache.spark.sql.Dataset$$anonfun$head$1.apply(Dataset.scala:2545)\n\tat org.apache.spark.sql.Dataset$$anonfun$53.apply(Dataset.scala:3365)\n\tat org.apache.spark.sql.execution.SQLExecution$$anonfun$withNewExecutionId$1.apply(SQLExecution.scala:78)\n\tat org.apache.spark.sql.execution.SQLExecution$.withSQLConfPropagated(SQLExecution.scala:125)\n\tat org.apache.spark.sql.execution.SQLExecution$.withNewExecutionId(SQLExecution.scala:73)\n\tat org.apache.spark.sql.Dataset.withAction(Dataset.scala:3364)\n\tat org.apache.spark.sql.Dataset.head(Dataset.scala:2545)\n\tat org.apache.spark.sql.Dataset.take(Dataset.scala:2759)\n\tat org.apache.spark.sql.Dataset.getRows(Dataset.scala:255)\n\tat org.apache.spark.sql.Dataset.showString(Dataset.scala:292)\n\tat sun.reflect.GeneratedMethodAccessor61.invoke(Unknown Source)\n\tat sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n\tat java.lang.reflect.Method.invoke(Method.java:498)\n\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\n\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357)\n\tat py4j.Gateway.invoke(Gateway.java:282)\n\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\n\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\n\tat py4j.GatewayConnection.run(GatewayConnection.java:238)\n\tat java.lang.Thread.run(Thread.java:748)\nCaused by: org.apache.spark.api.python.PythonException: Traceback (most recent call last):\n  File \"/Users/baiyue/spark-2.4.0-bin-hadoop2.7/python/lib/pyspark.zip/pyspark/worker.py\", line 372, in main\n    process()\n  File \"/Users/baiyue/spark-2.4.0-bin-hadoop2.7/python/lib/pyspark.zip/pyspark/worker.py\", line 367, in process\n    serializer.dump_stream(func(split_index, iterator), outfile)\n  File \"/Users/baiyue/spark-2.4.0-bin-hadoop2.7/python/lib/pyspark.zip/pyspark/serializers.py\", line 342, in dump_stream\n    self.serializer.dump_stream(self._batched(iterator), stream)\n  File \"/Users/baiyue/spark-2.4.0-bin-hadoop2.7/python/lib/pyspark.zip/pyspark/serializers.py\", line 141, in dump_stream\n    for obj in iterator:\n  File \"/Users/baiyue/spark-2.4.0-bin-hadoop2.7/python/lib/pyspark.zip/pyspark/serializers.py\", line 331, in _batched\n    for item in iterator:\n  File \"<string>\", line 1, in <lambda>\n  File \"/Users/baiyue/spark-2.4.0-bin-hadoop2.7/python/lib/pyspark.zip/pyspark/worker.py\", line 80, in <lambda>\n    return lambda *a: f(*a)\n  File \"/Users/baiyue/spark-2.4.0-bin-hadoop2.7/python/lib/pyspark.zip/pyspark/util.py\", line 99, in wrapper\n    return f(*args, **kwargs)\n  File \"<ipython-input-162-90c40e18ab34>\", line 3, in Larger\nTypeError: '>' not supported between instances of 'float' and 'str'\n\n\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.handlePythonException(PythonRunner.scala:452)\n\tat org.apache.spark.sql.execution.python.PythonUDFRunner$$anon$1.read(PythonUDFRunner.scala:81)\n\tat org.apache.spark.sql.execution.python.PythonUDFRunner$$anon$1.read(PythonUDFRunner.scala:64)\n\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.hasNext(PythonRunner.scala:406)\n\tat org.apache.spark.InterruptibleIterator.hasNext(InterruptibleIterator.scala:37)\n\tat scala.collection.Iterator$$anon$12.hasNext(Iterator.scala:440)\n\tat scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:409)\n\tat scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:409)\n\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage5.agg_doAggregateWithoutKey_0$(Unknown Source)\n\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage5.processNext(Unknown Source)\n\tat org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43)\n\tat org.apache.spark.sql.execution.WholeStageCodegenExec$$anonfun$11$$anon$1.hasNext(WholeStageCodegenExec.scala:619)\n\tat scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:409)\n\tat org.apache.spark.shuffle.sort.BypassMergeSortShuffleWriter.write(BypassMergeSortShuffleWriter.java:125)\n\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:99)\n\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:55)\n\tat org.apache.spark.scheduler.Task.run(Task.scala:121)\n\tat org.apache.spark.executor.Executor$TaskRunner$$anonfun$10.apply(Executor.scala:402)\n\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1360)\n\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:408)\n\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n\t... 1 more\n"
     ]
    }
   ],
   "source": [
    "df.select('maxfar', 'maxfar_new')\\\n",
    "  .withColumn('Larger', Larger(col('maxfar'),col('maxfar_new')))\\\n",
    "  .withColumn('Equal', Equal(col('maxfar'),col('maxfar_new')))\\\n",
    "  .withColumn('Less', Less(col('maxfar'),col('maxfar_new')))\\\n",
    "  .agg(sum(col('Larger')).alias('decrease'),\n",
    "      sum(col('Equal')).alias('no change'),\n",
    "      sum(col('Less')).alias('increase')).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+\n",
      "|decrease|\n",
      "+--------+\n",
      "|  315182|\n",
      "+--------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.select('maxfar', 'maxfar_new')\\\n",
    "  .filter(col('maxfar') > col('maxfar_new')).agg(count('maxfar_new').alias('decrease')).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------------------+\n",
      "|decrease but over 0.5|\n",
      "+---------------------+\n",
      "|                    0|\n",
      "+---------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.select('maxfar', 'maxfar_new')\\\n",
    "  .filter((col('maxfar') > col('maxfar_new')) & \n",
    "          (col('maxfar_new') < 0.5)).agg(count('maxfar_new').alias('decrease but over 0.5')).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 145,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------------------+\n",
      "|decrease but over 0.5|\n",
      "+---------------------+\n",
      "|               133292|\n",
      "+---------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.select('maxfar', 'maxfar_new')\\\n",
    "  .filter((col('maxfar') > col('maxfar_new')) & \n",
    "          (col('maxfar_new') > 0.5)).agg(count('maxfar_new').alias('decrease but over 0.5')).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+------+----------+\n",
      "|zonedist1|maxfar|maxfar_new|\n",
      "+---------+------+----------+\n",
      "|R3-2     |0.6   |1.0       |\n",
      "|M1-6/R9  |0.0   |7.52      |\n",
      "|R7-1     |3.44  |4.0       |\n",
      "|R3-2     |0.6   |2.0       |\n",
      "|R5       |1.25  |3.0       |\n",
      "|R8B      |4.0   |6.0       |\n",
      "|C8-3     |2.0   |3.44      |\n",
      "|R3A      |0.6   |1.0       |\n",
      "|C8-2     |2.0   |3.0       |\n",
      "|R7-1     |3.44  |5.0       |\n",
      "|R5D      |2.0   |3.4       |\n",
      "|R2A      |0.5   |2.0       |\n",
      "|PARK     |0.0   |3.0       |\n",
      "|PARK     |0.0   |0.75      |\n",
      "|R5B      |1.35  |4.0       |\n",
      "|M1-2     |2.0   |6.02      |\n",
      "|R3X      |0.6   |2.0       |\n",
      "|R6B      |2.0   |3.4       |\n",
      "|M1-1     |1.0   |1.35      |\n",
      "|R6B      |2.0   |4.2       |\n",
      "|M1-2     |2.0   |2.43      |\n",
      "|R3-1     |0.6   |1.0       |\n",
      "|C8-1     |1.0   |2.0       |\n",
      "|R5       |1.25  |2.0       |\n",
      "|R3A      |0.6   |3.4       |\n",
      "|M1-4     |2.0   |4.0       |\n",
      "|R6       |2.43  |3.4       |\n",
      "|R5B      |1.35  |3.4       |\n",
      "|R2       |0.5   |1.0       |\n",
      "|R3-2     |0.6   |3.0       |\n",
      "|M2-1     |2.0   |2.43      |\n",
      "|R3-2     |0.6   |3.4       |\n",
      "|C8-3     |2.0   |6.02      |\n",
      "|R3X      |0.6   |3.4       |\n",
      "|R6B      |2.0   |3.0       |\n",
      "|R10      |10.0  |15.0      |\n",
      "|M1-4/R7D |2.0   |4.2       |\n",
      "|PARK     |0.0   |0.5       |\n",
      "|R2A      |0.5   |1.0       |\n",
      "|R5       |1.25  |4.0       |\n",
      "|M1-1     |1.0   |3.44      |\n",
      "|C8-1     |1.0   |3.44      |\n",
      "|R2       |0.5   |3.4       |\n",
      "|C8-1     |1.0   |1.35      |\n",
      "|M1-2     |2.0   |3.0       |\n",
      "|M1-4D    |2.0   |3.0       |\n",
      "|R4A      |0.9   |1.0       |\n",
      "|R6A      |3.0   |3.4       |\n",
      "|R6       |2.43  |4.0       |\n",
      "|R6       |2.43  |4.2       |\n",
      "|C8-1     |1.0   |2.43      |\n",
      "|R6A      |3.0   |4.0       |\n",
      "|R5B      |1.35  |2.0       |\n",
      "|R4       |0.9   |2.0       |\n",
      "|R6B      |2.0   |6.0       |\n",
      "|M1-4/R7X |3.75  |5.0       |\n",
      "|C8-2     |2.0   |3.44      |\n",
      "|PARK     |0.0   |2.0       |\n",
      "|R8A      |6.0   |6.02      |\n",
      "|M1-1     |1.0   |4.2       |\n",
      "|M3-1     |2.0   |2.43      |\n",
      "|M1-4     |2.0   |5.0       |\n",
      "|R4B      |0.9   |2.0       |\n",
      "|R5       |1.25  |3.4       |\n",
      "|M1-1/R6A |1.0   |3.0       |\n",
      "|M1-4     |2.0   |3.0       |\n",
      "|C8-2     |2.0   |2.43      |\n",
      "|M1-1     |1.0   |2.43      |\n",
      "|M1-1     |1.0   |3.0       |\n",
      "|R6       |2.43  |5.0       |\n",
      "|R4-1     |0.9   |1.0       |\n",
      "|R4       |0.9   |1.0       |\n",
      "|M1-5     |5.0   |7.52      |\n",
      "|M1-1     |1.0   |5.0       |\n",
      "|M1-1     |1.0   |6.02      |\n",
      "|R6B      |2.0   |4.0       |\n",
      "|PARK     |0.0   |2.43      |\n",
      "|C8-1     |1.0   |1.1       |\n",
      "|R7-2     |3.44  |6.0       |\n",
      "|R6       |2.43  |6.0       |\n",
      "|R7A      |4.0   |6.0       |\n",
      "|R5B      |1.35  |3.0       |\n",
      "|C8-2     |2.0   |6.02      |\n",
      "|R8       |6.02  |9.0       |\n",
      "|PARK     |0.0   |6.02      |\n",
      "|M1-1     |1.0   |2.0       |\n",
      "|R4B      |0.9   |3.0       |\n",
      "|C8-1     |1.0   |1.25      |\n",
      "|R4       |0.9   |3.4       |\n",
      "|M1-4/R8A |5.4   |6.02      |\n",
      "|M1-2     |2.0   |4.0       |\n",
      "|PARK     |0.0   |1.0       |\n",
      "|R4B      |0.9   |1.0       |\n",
      "|M1-1     |1.0   |4.0       |\n",
      "|R4A      |0.9   |3.4       |\n",
      "|C8-1     |1.0   |3.0       |\n",
      "|M1-1     |1.0   |1.25      |\n",
      "|R3X      |0.6   |1.0       |\n",
      "|PARK     |0.0   |3.44      |\n",
      "|C8-4     |5.0   |6.02      |\n",
      "+---------+------+----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.filter(col('maxfar') < col('maxfar_new'))\\\n",
    "    .select('zonedist1', 'maxfar', 'maxfar_new').distinct().show(df.count(), False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+------+\n",
      "|zonedist1|counts|\n",
      "+---------+------+\n",
      "|R8A      |591   |\n",
      "|M1-1     |133   |\n",
      "|R3-2     |79    |\n",
      "|R6B      |71    |\n",
      "|R3A      |66    |\n",
      "|R5       |59    |\n",
      "|PARK     |52    |\n",
      "|R4       |51    |\n",
      "|C8-1     |51    |\n",
      "|R6       |43    |\n",
      "|C8-2     |36    |\n",
      "|R3X      |36    |\n",
      "|M1-4/R8A |20    |\n",
      "|R4A      |20    |\n",
      "|R4B      |19    |\n",
      "|M1-6/R9  |18    |\n",
      "|R5B      |18    |\n",
      "|R2       |13    |\n",
      "|R7-1     |11    |\n",
      "|C8-3     |11    |\n",
      "|R6A      |10    |\n",
      "|M1-2     |10    |\n",
      "|R3-1     |8     |\n",
      "|R4-1     |7     |\n",
      "|M1-4/R7X |7     |\n",
      "|M1-4/R7D |5     |\n",
      "|R2A      |4     |\n",
      "|M1-5     |4     |\n",
      "|M1-4     |3     |\n",
      "|C8-4     |2     |\n",
      "|R7-2     |2     |\n",
      "|R8B      |2     |\n",
      "|M1-1/R6A |2     |\n",
      "|M3-1     |2     |\n",
      "|R8       |1     |\n",
      "|M1-4D    |1     |\n",
      "|R10      |1     |\n",
      "|R7A      |1     |\n",
      "|R5D      |1     |\n",
      "|M2-1     |1     |\n",
      "+---------+------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.filter(col('maxfar') < col('maxfar_new'))\\\n",
    "    .groupBy('zonedist1')\\\n",
    "    .agg(count('zonedist1').alias('counts'))\\\n",
    "    .sort(col('counts').desc()).show(df.count(), False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------------------+\n",
      "|count(pctunbuilt_new)|\n",
      "+---------------------+\n",
      "|               304494|\n",
      "+---------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.select('residfar_new', 'commfar_new', 'pctunbuilt', 'pctunbuilt_new')\\\n",
    "  .filter(col('pctunbuilt') > col('pctunbuilt_new')).agg(count('pctunbuilt_new')).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------------+------------------+------------------+\n",
      "|   avg(maxfar_new)|       avg(maxfar)|     avg(builtfar)|\n",
      "+------------------+------------------+------------------+\n",
      "|1.1085407715997209|1.1666834392287233|0.8391015326831988|\n",
      "+------------------+------------------+------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.filter(col('zonedist1').like('R%'))\\\n",
    "   .select('maxfar_new', 'maxfar', 'builtfar')\\\n",
    "   .agg(mean('maxfar_new'),mean('maxfar'),mean('builtfar')).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+-------------+---------+----+\n",
      "|borough|softsites_new|softsites|diff|\n",
      "+-------+-------------+---------+----+\n",
      "|     SI|        95252|    95226|  26|\n",
      "|     MN|        14265|    14246|  19|\n",
      "|     QN|       268405|   268392|  13|\n",
      "|     BK|       229191|   229183|   8|\n",
      "|     BX|        64052|    64049|   3|\n",
      "+-------+-------------+---------+----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# By Borough Comparison\n",
    "df.groupBy('borough')\\\n",
    "    .agg(count(col('pctunbuilt_new') >= 0.5).alias('softsites_new'),\n",
    "         count(col('pctunbuilt') >= 0.5).alias('softsites'))\\\n",
    "    .withColumn('diff', col('softsites_new')-col('softsites'))\\\n",
    "    .sort(col('diff').desc()).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+-------------+---------+----+\n",
      "| cd|softsites_new|softsites|diff|\n",
      "+---+-------------+---------+----+\n",
      "|503|        37489|    37467|  22|\n",
      "|111|         1514|     1496|  18|\n",
      "|302|         5126|     5122|   4|\n",
      "|595|           18|       16|   2|\n",
      "|307|        11232|    11230|   2|\n",
      "|404|         9101|     9099|   2|\n",
      "|305|        17627|    17625|   2|\n",
      "|413|        37981|    37979|   2|\n",
      "|484|           10|        8|   2|\n",
      "|501|        31319|    31317|   2|\n",
      "|411|        20874|    20872|   2|\n",
      "|409|        18288|    18287|   1|\n",
      "|208|         2589|     2588|   1|\n",
      "|414|         9996|     9995|   1|\n",
      "|408|        15945|    15944|   1|\n",
      "|481|            6|        5|   1|\n",
      "|110|         2056|     2055|   1|\n",
      "|407|        28144|    28143|   1|\n",
      "|103|          899|      898|   1|\n",
      "|201|         2350|     2349|   1|\n",
      "+---+-------------+---------+----+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.groupBy('cd')\\\n",
    "    .agg(count(col('pctunbuilt_new') >= 0.5).alias('softsites_new'),\n",
    "         count(col('pctunbuilt') >= 0.5).alias('softsites'))\\\n",
    "    .withColumn('diff', col('softsites_new')-col('softsites'))\\\n",
    "    .sort(col('diff').desc()).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
